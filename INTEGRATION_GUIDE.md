# æ•°æ®çˆ¬è™«æ¨¡å—é›†æˆæŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•ä½¿ç”¨ `feature/data-crawler-integration` åˆ†æ”¯é›†æˆçš„æ•°æ®é‡‡é›†åŠŸèƒ½ã€‚

## åˆ†æ”¯è¯´æ˜

### main åˆ†æ”¯
- **å®šä½**ï¼šå®æ—¶ç›‘æ§ + é€šçŸ¥ç³»ç»Ÿ
- **åŠŸèƒ½**ï¼šLOFç›‘æ§ã€Aè‚¡/ç¾è‚¡æ—¥æŠ¥ã€RSS AIåˆ†æ
- **è¾“å‡º**ï¼šé’‰é’‰é€šçŸ¥
- **å­˜å‚¨**ï¼šJSONçŠ¶æ€æ–‡ä»¶

### feature/data-crawler-integration åˆ†æ”¯
- **å®šä½**ï¼šmainåˆ†æ”¯ + æ•°æ®é‡‡é›†æ¨¡å—
- **æ–°å¢**ï¼šå†å²æ•°æ®å­˜å‚¨ã€æ–°é—»çˆ¬è™«ã€æœŸè´§ç§»ä»“ä¿¡å·
- **è¾“å‡º**ï¼šé’‰é’‰é€šçŸ¥ + MySQLæ•°æ®åº“
- **å­˜å‚¨**ï¼šJSON + MySQL

## å¿«é€Ÿå¼€å§‹

### 1. åˆ‡æ¢åˆ°é›†æˆåˆ†æ”¯

```bash
# æŸ¥çœ‹æ‰€æœ‰åˆ†æ”¯
git branch -a

# åˆ‡æ¢åˆ°é›†æˆåˆ†æ”¯
git checkout feature/data-crawler-integration

# æŸ¥çœ‹æ–°å¢çš„ç›®å½•
ls -la data_crawler/
```

### 2. æŸ¥çœ‹æ–°å¢åŠŸèƒ½

```bash
# æŸ¥çœ‹æ•°æ®çˆ¬è™«æ¨¡å—æ–‡æ¡£
cat data_crawler/README.md

# æŸ¥çœ‹ç›®å½•ç»“æ„
tree data_crawler/
```

### 3. é…ç½®æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦ä½¿ç”¨æ•°æ®é‡‡é›†åŠŸèƒ½ï¼š

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡é…ç½®
cd data_crawler
cp .env.example .env

# ç¼–è¾‘æ•°æ®åº“é…ç½®
vim .env
```

é…ç½®å†…å®¹ï¼š
```bash
DB_HOST=your_mysql_host
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_password
DB_NAME=watchdog_db
TIMEZONE=Asia/Shanghai
LOG_LEVEL=INFO
```

### 4. åˆå§‹åŒ–æ•°æ®åº“è¡¨

```bash
cd data_crawler
pip install -r requirements.txt

# åˆ›å»ºæ•°æ®åº“è¡¨
python -c "from db.init_tables import init_all_tables; init_all_tables()"
```

### 5. è¿è¡Œæ•°æ®é‡‡é›†å™¨

```bash
# æ–¹å¼1ï¼šç›´æ¥è¿è¡Œ
python scheduler/main.py

# æ–¹å¼2ï¼šä½¿ç”¨Docker
docker-compose -f docker-compose.crawler.yml up -d
```

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šä»…ä½¿ç”¨é€šçŸ¥åŠŸèƒ½ï¼ˆmainåˆ†æ”¯ï¼‰

**é€‚åˆ**ï¼šä¸ªäººæŠ•èµ„è€…ï¼Œåªéœ€è¦å®æ—¶å‘Šè­¦

```bash
git checkout main
docker-compose up -d
```

**ç‰¹ç‚¹**ï¼š
- âœ… è½»é‡çº§ï¼Œæ— æ•°æ®åº“
- âœ… éƒ¨ç½²ç®€å•
- âœ… LOF/Aè‚¡/ç¾è‚¡å®æ—¶ç›‘æ§
- âœ… AIåˆ†æRSSæ–‡ç« 
- âŒ æ— å†å²æ•°æ®

### åœºæ™¯2ï¼šé€šçŸ¥ + æ•°æ®é‡‡é›†ï¼ˆé›†æˆåˆ†æ”¯ï¼‰

**é€‚åˆ**ï¼šé‡åŒ–ç ”ç©¶å‘˜ï¼Œéœ€è¦å†å²æ•°æ®åˆ†æ

```bash
git checkout feature/data-crawler-integration

# è¿è¡Œä¸»ç›‘æ§ç³»ç»Ÿï¼ˆé’‰é’‰é€šçŸ¥ï¼‰
docker-compose up -d

# è¿è¡Œæ•°æ®é‡‡é›†å™¨ï¼ˆMySQLå­˜å‚¨ï¼‰
cd data_crawler
docker-compose -f docker-compose.crawler.yml up -d
```

**ç‰¹ç‚¹**ï¼š
- âœ… å®Œæ•´çš„é€šçŸ¥åŠŸèƒ½
- âœ… å†å²æ•°æ®å­˜å‚¨ï¼ˆMySQLï¼‰
- âœ… æ–°é—»è‡ªåŠ¨é‡‡é›†
- âœ… æœŸè´§ç§»ä»“ä¿¡å·
- âš ï¸ éœ€è¦MySQLæ•°æ®åº“

### åœºæ™¯3ï¼šä»…æ•°æ®é‡‡é›†ï¼ˆæ— é€šçŸ¥ï¼‰

**é€‚åˆ**ï¼šæ„å»ºæ•°æ®ä»“åº“ï¼Œåç»­åˆ†æä½¿ç”¨

```bash
git checkout feature/data-crawler-integration
cd data_crawler
python scheduler/main.py
```

**ç‰¹ç‚¹**ï¼š
- âœ… ä¸“æ³¨æ•°æ®é‡‡é›†
- âœ… è½»é‡çº§è¿è¡Œ
- âŒ æ— å®æ—¶å‘Šè­¦

## æ•°æ®æµå‘

### Mainåˆ†æ”¯æ•°æ®æµ
```
æ•°æ®æº â†’ å®æ—¶ç›‘æ§ â†’ é’‰é’‰é€šçŸ¥
         â†“
      state.json
```

### é›†æˆåˆ†æ”¯æ•°æ®æµ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¸»ç›‘æ§ç³»ç»Ÿ      â”‚  â†’ é’‰é’‰é€šçŸ¥
â”‚  (mainåŠŸèƒ½)     â”‚  â†’ state.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ•°æ®é‡‡é›†æ¨¡å—    â”‚  â†’ MySQLæ•°æ®åº“
â”‚  (data_crawler) â”‚     â”œâ”€ æ–°é—»è¡¨
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”œâ”€ Kçº¿è¡¨
                        â”œâ”€ å®æ—¶ä»·æ ¼è¡¨
                        â””â”€ æœŸè´§ä¿¡å·è¡¨
```

### å®Œæ•´é›†æˆæ•°æ®æµï¼ˆæ¨èï¼‰
```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ æ•°æ®æºAPI     â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ä¸»ç›‘æ§ç³»ç»Ÿâ”‚      â”‚æ•°æ®é‡‡é›†æ¨¡å—   â”‚
  â”‚(å®æ—¶å‘Šè­¦)â”‚      â”‚(å†å²å­˜å‚¨)     â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â–¼                   â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚é’‰é’‰é€šçŸ¥ â”‚      â”‚MySQLæ•°æ®åº“    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚æ•°æ®åˆ†æ/å›æµ‹  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## åŠŸèƒ½å¯¹æ¯”

| åŠŸèƒ½ | mainåˆ†æ”¯ | é›†æˆåˆ†æ”¯ |
|------|---------|---------|
| LOFå®æ—¶ç›‘æ§ | âœ… | âœ… |
| Aè‚¡å¸‚åœºæ—¥æŠ¥ | âœ… | âœ… |
| ç¾è‚¡å¸‚åœºæ—¥æŠ¥ | âœ… | âœ… |
| RSS AIåˆ†æ | âœ… | âœ… |
| é’‰é’‰é€šçŸ¥ | âœ… | âœ… |
| **æ–°é—»é‡‡é›†** | âŒ | âœ… |
| **å†å²Kçº¿æ•°æ®** | âŒ | âœ… |
| **æœŸè´§ç§»ä»“ä¿¡å·** | âŒ | âœ… |
| **å®æ—¶ä»·æ ¼å¿«ç…§** | âŒ | âœ… |
| **MySQLå­˜å‚¨** | âŒ | âœ… |

## åˆå¹¶ç­–ç•¥å»ºè®®

### æ–¹æ¡ˆAï¼šä¿æŒåˆ†æ”¯ç‹¬ç«‹ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
# mainåˆ†æ”¯ç”¨äºç”Ÿäº§ç¯å¢ƒï¼ˆç¨³å®šï¼‰
git checkout main

# é›†æˆåˆ†æ”¯ç”¨äºæµ‹è¯•æ–°åŠŸèƒ½
git checkout feature/data-crawler-integration
```

**ä¼˜ç‚¹**ï¼š
- äº’ä¸å½±å“ï¼Œé£é™©éš”ç¦»
- å¯ä»¥éšæ—¶åˆ‡æ¢
- é€‚åˆè¯„ä¼°é˜¶æ®µ

### æ–¹æ¡ˆBï¼šåˆå¹¶åˆ°mainï¼ˆæ¨èé«˜çº§ç”¨æˆ·ï¼‰

```bash
# ç¡®è®¤é›†æˆåˆ†æ”¯åŠŸèƒ½æ­£å¸¸
git checkout feature/data-crawler-integration
# ... æµ‹è¯• ...

# åˆå¹¶åˆ°main
git checkout main
git merge feature/data-crawler-integration

# è§£å†³å¯èƒ½çš„å†²çª
git status
```

**ä¼˜ç‚¹**ï¼š
- ç»Ÿä¸€ç®¡ç†
- åŠŸèƒ½å®Œæ•´
- é€‚åˆé•¿æœŸä½¿ç”¨

### æ–¹æ¡ˆCï¼šé€‰æ‹©æ€§é›†æˆï¼ˆæ¨èå®šåˆ¶åŒ–ï¼‰

åªé›†æˆéœ€è¦çš„åŠŸèƒ½æ¨¡å—ï¼š

```bash
git checkout main

# åªå¤åˆ¶æœŸè´§çˆ¬è™«
git checkout feature/data-crawler-integration -- data_crawler/crawlers/futures_crawler.py
git checkout feature/data-crawler-integration -- data_crawler/db/

# è‡ªè¡Œè°ƒæ•´é›†æˆåˆ°ä¸»é¡¹ç›®
```

## é…ç½®æ–‡ä»¶è¯´æ˜

### ä¸»é¡¹ç›®é…ç½®ï¼ˆdata/config.jsonï¼‰
```json
{
  "dingtalk": {...},      // é’‰é’‰é€šçŸ¥é…ç½®
  "lof": {...},           // LOFç›‘æ§é…ç½®
  "ai": {...},            // AIåˆ†æé…ç½®
  "rss": {...}            // RSSè®¢é˜…é…ç½®
}
```

### æ•°æ®é‡‡é›†é…ç½®ï¼ˆdata_crawler/.envï¼‰
```bash
DB_HOST=...             # æ•°æ®åº“ä¸»æœº
DB_PASSWORD=...         # æ•°æ®åº“å¯†ç 
TIMEZONE=Asia/Shanghai  # æ—¶åŒº
LOG_LEVEL=INFO          # æ—¥å¿—çº§åˆ«
```

## Dockeréƒ¨ç½²

### å•ç‹¬éƒ¨ç½²

```bash
# ä¸»ç›‘æ§ç³»ç»Ÿ
docker-compose up -d

# æ•°æ®é‡‡é›†æ¨¡å—
cd data_crawler
docker-compose -f docker-compose.crawler.yml up -d
```

### ç»Ÿä¸€éƒ¨ç½²ï¼ˆéœ€è‡ªè¡Œé…ç½®ï¼‰

åˆ›å»º `docker-compose.unified.yml`ï¼š
```yaml
version: '3.8'

services:
  # ä¸»ç›‘æ§ç³»ç»Ÿ
  watchdog:
    build: .
    container_name: investment_watchdog
    restart: unless-stopped
    volumes:
      - ./data:/app/data
    environment:
      - TZ=Asia/Shanghai

  # æ•°æ®é‡‡é›†æ¨¡å—
  crawler:
    build:
      context: data_crawler
      dockerfile: Dockerfile.crawler
    container_name: data_crawler
    restart: unless-stopped
    env_file:
      - data_crawler/.env
    volumes:
      - ./data_crawler/logs:/app/logs
    depends_on:
      - mysql

  # MySQLæ•°æ®åº“
  mysql:
    image: mysql:8.0
    container_name: watchdog_mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: your_password
      MYSQL_DATABASE: watchdog_db
    volumes:
      - mysql_data:/var/lib/mysql
    ports:
      - "3306:3306"

volumes:
  mysql_data:
```

## æ•°æ®æŸ¥è¯¢ç¤ºä¾‹

### æŸ¥è¯¢æœ€æ–°æ–°é—»

```sql
SELECT title, source, publish_time
FROM news
ORDER BY created_at DESC
LIMIT 10;
```

### æŸ¥è¯¢æŒ‡æ•°Kçº¿

```sql
SELECT index_code, trade_date, close_price, change_pct
FROM index_daily_kline
WHERE index_code = 'SHCI'
  AND trade_date >= '2024-01-01'
ORDER BY trade_date DESC;
```

### æŸ¥è¯¢æœŸè´§ç§»ä»“ä¿¡å·

```sql
SELECT contract_type, check_date,
       volume_ratio, oi_ratio,
       rollover_signal, signal_reason
FROM futures_rollover
WHERE rollover_signal = 1
ORDER BY check_date DESC;
```

### æŸ¥è¯¢å®æ—¶ä»·æ ¼è¶‹åŠ¿

```sql
SELECT symbol, record_time, price, change_24h
FROM realtime_prices
WHERE symbol = 'BTC'
  AND record_time >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
ORDER BY record_time;
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šæ•°æ®é‡‡é›†å™¨å¯åŠ¨å¤±è´¥

```bash
# æ£€æŸ¥MySQLè¿æ¥
python -c "from data_crawler.db.connection import execute_query; execute_query('SELECT 1')"

# æŸ¥çœ‹æ—¥å¿—
tail -f data_crawler/logs/watchdog.log
```

### é—®é¢˜2ï¼šä¸»ç›‘æ§å’Œæ•°æ®é‡‡é›†å†²çª

å¦‚æœä¸¤ä¸ªç³»ç»Ÿéƒ½åœ¨æŠ“å–ç›¸åŒæ•°æ®æºï¼š
- è°ƒæ•´å®šæ—¶ä»»åŠ¡é¿å…åŒæ—¶æ‰§è¡Œ
- ä½¿ç”¨ä¸åŒçš„æ•°æ®æºAPI
- å¢åŠ è¯·æ±‚å»¶è¿Ÿ

### é—®é¢˜3ï¼šMySQLå­˜å‚¨ç©ºé—´ä¸è¶³

```sql
-- æŸ¥çœ‹è¡¨å¤§å°
SELECT
    table_name,
    ROUND(((data_length + index_length) / 1024 / 1024), 2) AS 'Size (MB)'
FROM information_schema.tables
WHERE table_schema = 'watchdog_db'
ORDER BY (data_length + index_length) DESC;

-- å®šæœŸæ¸…ç†æ—§æ•°æ®
DELETE FROM news WHERE created_at < DATE_SUB(NOW(), INTERVAL 3 MONTH);
DELETE FROM realtime_prices WHERE record_time < DATE_SUB(NOW(), INTERVAL 1 MONTH);
```

## åç»­å¼€å‘å»ºè®®

### çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰
1. âœ… æµ‹è¯•æ•°æ®é‡‡é›†æ¨¡å—ç¨³å®šæ€§
2. âœ… ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼ˆæ·»åŠ ç´¢å¼•ï¼‰
3. âœ… é›†æˆæœŸè´§ç§»ä»“ä¿¡å·åˆ°é’‰é’‰é€šçŸ¥

### ä¸­æœŸï¼ˆ1ä¸ªæœˆï¼‰
1. ğŸ“Š å¼€å‘æ•°æ®åˆ†ææ¥å£
2. ğŸ“ˆ æ·»åŠ Kçº¿å›¾è¡¨ç”Ÿæˆ
3. ğŸ”” åŸºäºå†å²æ•°æ®çš„å¼‚å¸¸æ£€æµ‹å‘Šè­¦

### é•¿æœŸï¼ˆ3ä¸ªæœˆï¼‰
1. ğŸ¤– æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹
2. ğŸ“± Webå¯è§†åŒ–ç•Œé¢
3. ğŸ”„ ç­–ç•¥å›æµ‹ç³»ç»Ÿ

## æ€»ç»“

- **mainåˆ†æ”¯**ï¼šè½»é‡çº§ï¼Œä¸“æ³¨å®æ—¶é€šçŸ¥
- **é›†æˆåˆ†æ”¯**ï¼šå®Œæ•´åŠŸèƒ½ï¼ŒåŒ…å«æ•°æ®å­˜å‚¨
- **æ¨è**ï¼šå…ˆæµ‹è¯•é›†æˆåˆ†æ”¯ï¼Œç¨³å®šååˆå¹¶æˆ–ç‹¬ç«‹ä½¿ç”¨

é€‰æ‹©é€‚åˆä½ éœ€æ±‚çš„æ–¹æ¡ˆï¼Œå¼€å§‹ä½¿ç”¨å§ï¼ğŸš€
